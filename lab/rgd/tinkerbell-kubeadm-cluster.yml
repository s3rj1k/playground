apiVersion: kro.run/v1alpha1
kind: ResourceGraphDefinition
metadata:
  name: tinkerbellkubeadmcluster.v1alpha1.kro.run
spec:
  schema:
    apiVersion: v1alpha1
    kind: TinkerbellKubeadmCluster
    spec:
      name: string
      kubernetesVersion: string | default=v1.34.3
      controlPlaneEndpoint:
        host: string
        port: integer | default=6443
      network:
        podCIDR: string | default=192.168.0.0/16
        serviceCIDR: string | default=172.26.0.0/16
      controlPlane:
        replicas: integer | default=1
      workers:
        replicas: integer | default=1
      image:
        registry: string | default=ghcr.io/s3rj1k/playground
        osDistro: string | default=ubuntu
        osVersion: string | default="2404"
      kubeVip:
        version: string | default=v1.0.3
      user:
        name: string | default=tink
        passwordHash: string | default="!"
        lockPassword: boolean | default=false
        sshAuthorizedKeys: "[]string | default=[]"
    status:
      clusterReady: ${tinkerbellCluster.status.ready}
  resources:
    # Cilium HelmChartProxy
    - id: ciliumAddon
      template:
        apiVersion: addons.cluster.x-k8s.io/v1alpha1
        kind: HelmChartProxy
        metadata:
          name: cilium-${schema.spec.name}
          namespace: ${schema.metadata.namespace}
        spec:
          clusterSelector:
            matchLabels:
              cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
          repoURL: https://helm.cilium.io
          chartName: cilium
          namespace: kube-system
          releaseName: cilium
          valuesTemplate: |
            ipam:
              mode: kubernetes
            k8sServiceHost: {{ .Cluster.spec.controlPlaneEndpoint.host }}
            k8sServicePort: {{ .Cluster.spec.controlPlaneEndpoint.port }}
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
              - key: node.cluster.x-k8s.io/uninitialized
                operator: Exists
                effect: NoSchedule
              - key: node.kubernetes.io/not-ready
                operator: Exists
                effect: NoSchedule
            operator:
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                - key: node.cluster.x-k8s.io/uninitialized
                  operator: Exists
                  effect: NoSchedule
                - key: node.kubernetes.io/not-ready
                  operator: Exists
                  effect: NoSchedule
            envoy:
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                - key: node.cluster.x-k8s.io/uninitialized
                  operator: Exists
                  effect: NoSchedule
                - key: node.kubernetes.io/not-ready
                  operator: Exists
                  effect: NoSchedule

    # KubeadmControlPlane
    - id: controlPlane
      template:
        apiVersion: controlplane.cluster.x-k8s.io/v1beta2
        kind: KubeadmControlPlane
        metadata:
          name: ${schema.spec.name}-control-plane
          namespace: ${schema.metadata.namespace}
        spec:
          kubeadmConfigSpec:
            clusterConfiguration:
              controlPlaneEndpoint: "${schema.spec.controlPlaneEndpoint.host}.nip.io:${string(schema.spec.controlPlaneEndpoint.port)}"
              apiServer:
                certSANs:
                  - "${schema.spec.controlPlaneEndpoint.host}"
                  - "${schema.spec.controlPlaneEndpoint.host}.nip.io"
              featureGates:
                ControlPlaneKubeletLocalMode: true
            initConfiguration:
              nodeRegistration:
                kubeletExtraArgs:
                  - name: provider-id
                    value: PROVIDER_ID
            joinConfiguration:
              nodeRegistration:
                ignorePreflightErrors:
                  - DirAvailable--etc-kubernetes-manifests
                kubeletExtraArgs:
                  - name: provider-id
                    value: PROVIDER_ID
            files:
              - path: /usr/local/bin/kube-vip-setup.sh
                permissions: "0755"
                content: |
                  #!/bin/bash
                  set -e

                  PHASE="$1"
                  KVVERSION="${schema.spec.kubeVip.version}"
                  KVIMAGE="ghcr.io/kube-vip/kube-vip:$KVVERSION"
                  VIP_ADDRESS="${schema.spec.controlPlaneEndpoint.host}"
                  VIP_HOSTNAME="${schema.spec.controlPlaneEndpoint.host}.nip.io"
                  KUBECONFIG_PATH="/etc/kubernetes/super-admin.conf"

                  setup_hosts_hack() {
                    if ! curl -sk --connect-timeout 2 "https://$VIP_HOSTNAME:6443/healthz" >/dev/null 2>&1; then
                      echo "127.0.0.1 $VIP_HOSTNAME" >> /etc/hosts
                    fi
                  }

                  remove_hosts_hack() {
                    sed -i "/$VIP_HOSTNAME/d" /etc/hosts
                  }

                  generate_manifest() {
                    "$1" manifest pod \
                      --arp \
                      --interface "" \
                      --address "$VIP_ADDRESS" \
                      --controlplane \
                      --leaderElection \
                      --k8sConfigPath "$KUBECONFIG_PATH"
                  }

                  setup_kube_vip() {
                    mkdir -p /etc/kubernetes/manifests
                    if command -v ctr >/dev/null 2>&1 && [ -S /run/containerd/containerd.sock ]; then
                      ctr image pull "$KVIMAGE"
                      ctr run --rm --net-host "$KVIMAGE" vip /kube-vip manifest pod \
                        --arp \
                        --interface "" \
                        --address "$VIP_ADDRESS" \
                        --controlplane \
                        --leaderElection \
                        --k8sConfigPath "$KUBECONFIG_PATH" \
                        > /etc/kubernetes/manifests/kube-vip.yaml
                    elif command -v crictl >/dev/null 2>&1; then
                      crictl pull "$KVIMAGE" >/dev/null 2>&1
                      KVBINARY=$(find /var/lib/containers/storage/overlay -name "kube-vip" -type f 2>/dev/null | head -1)
                      if [ -z "$KVBINARY" ]; then
                        echo "ERROR: kube-vip binary not found in container storage" >&2
                        exit 1
                      fi
                      generate_manifest "$KVBINARY" > /etc/kubernetes/manifests/kube-vip.yaml
                    else
                      echo "ERROR: No supported container runtime found (ctr or crictl)" >&2
                      exit 1
                    fi
                  }

                  case "$PHASE" in
                    pre)
                      if ! curl -sk --connect-timeout 2 "https://$VIP_HOSTNAME:6443/healthz" >/dev/null 2>&1; then
                        # First node - VIP not reachable, need hosts hack and kube-vip
                        setup_hosts_hack
                        setup_kube_vip
                      fi
                      ;;
                    post)
                      if curl -sk --connect-timeout 2 "https://$VIP_HOSTNAME:6443/healthz" >/dev/null 2>&1; then
                        # Joining node - VIP reachable, setup kube-vip now with admin.conf
                        KUBECONFIG_PATH="/etc/kubernetes/admin.conf"
                        setup_kube_vip
                      else
                        # First node - remove hosts hack
                        remove_hosts_hack
                      fi
                      ;;
                    *)
                      echo "Usage: $0 {pre|post}" >&2
                      exit 1
                      ;;
                  esac
            preKubeadmCommands:
              - /usr/local/bin/kube-vip-setup.sh pre
            postKubeadmCommands:
              - /usr/local/bin/kube-vip-setup.sh post
            users:
              - name: ${schema.spec.user.name}
                passwd: ${schema.spec.user.passwordHash}
                lockPassword: ${schema.spec.user.lockPassword}
                sshAuthorizedKeys: ${schema.spec.user.sshAuthorizedKeys}
                sudo: ALL=(ALL) NOPASSWD:ALL
                shell: /bin/bash
          machineTemplate:
            spec:
              infrastructureRef:
                apiGroup: infrastructure.cluster.x-k8s.io
                kind: TinkerbellMachineTemplate
                name: ${schema.spec.name}-control-plane
          replicas: ${schema.spec.controlPlane.replicas}
          version: ${schema.spec.kubernetesVersion}

    # Control Plane TinkerbellMachineTemplate
    - id: controlPlaneMachineTemplate
      template:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: TinkerbellMachineTemplate
        metadata:
          name: ${schema.spec.name}-control-plane
          namespace: ${schema.metadata.namespace}
        spec:
          template:
            spec:
              bootOptions:
                bootMode: customboot
                custombootConfig:
                  preparingActions:
                    - powerAction: "off"
                    - bootDevice:
                        device: "pxe"
                        efiBoot: true
                    - powerAction: "on"
                  postActions:
                    - powerAction: "off"
                    - bootDevice:
                        device: "disk"
                        persistent: true
                        efiBoot: true
                    - powerAction: "on"
              hardwareAffinity:
                required:
                  - labelSelector:
                      matchLabels:
                        tinkerbell.org/role: control-plane
              templateOverride: |-
                version: "0.1"
                name: control-plane
                global_timeout: 9000
                tasks:
                  - name: "control-plane"
                    worker: "{{.device_1}}"
                    volumes:
                      - /dev:/dev
                      - /dev/console:/dev/console
                      - /lib/firmware:/lib/firmware:ro
                    actions:
                      - name: "Stream Ubuntu Image"
                        image: quay.io/tinkerbell/actions/oci2disk:latest
                        timeout: 3000
                        environment:
                          DEST_DISK: {{ index .Hardware.Disks 0 }}
                          IMG_URL: ${schema.spec.image.registry}/${schema.spec.image.osDistro}-${schema.spec.image.osVersion}:${schema.spec.kubernetesVersion}.gz
                          COMPRESSED: true
                      - name: "Grow Partition"
                        image: quay.io/tinkerbell/actions/cexec:latest
                        timeout: 90
                        environment:
                          BLOCK_DEVICE: {{ index .Hardware.Disks 0 }}3
                          FS_TYPE: ext4
                          CHROOT: y
                          DEFAULT_INTERPRETER: "/bin/sh -c"
                          CMD_LINE: "growpart {{ index .Hardware.Disks 0 }} 3 && resize2fs {{ index .Hardware.Disks 0 }}3"
                      - name: "Add Tink Cloud-Init Config"
                        image: quay.io/tinkerbell/actions/writefile:latest
                        timeout: 90
                        environment:
                          DEST_DISK: {{ formatPartition ( index .Hardware.Disks 0 ) 3 }}
                          FS_TYPE: ext4
                          DEST_PATH: /etc/cloud/cloud.cfg.d/10_tinkerbell.cfg
                          UID: 0
                          GID: 0
                          MODE: 0600
                          DIRMODE: 0700
                          CONTENTS: |
                            datasource:
                              Ec2:
                                metadata_urls: ["http://{{ (index .Hardware.Interfaces 0).DHCP.IP.Gateway }}:7172"]
                                strict_id: false
                            manage_etc_hosts: localhost
                            warnings:
                              dsid_missing_source: off
                      - name: "Add Tink Cloud-Init DS-Identity"
                        image: quay.io/tinkerbell/actions/writefile:latest
                        timeout: 90
                        environment:
                          DEST_DISK: {{ formatPartition ( index .Hardware.Disks 0 ) 3 }}
                          FS_TYPE: ext4
                          DEST_PATH: /etc/cloud/ds-identify.cfg
                          UID: 0
                          GID: 0
                          MODE: 0600
                          DIRMODE: 0700
                          CONTENTS: |
                            datasource: Ec2
                      - name: "Shutdown"
                        image: ghcr.io/jacobweinstock/waitdaemon:latest
                        timeout: 90
                        pid: host
                        command: ["shutdown"]
                        environment:
                          IMAGE: alpine
                          WAIT_SECONDS: 10
                        volumes:
                          - /var/run/docker.sock:/var/run/docker.sock

    # Worker TinkerbellMachineTemplate
    - id: workerMachineTemplate
      template:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: TinkerbellMachineTemplate
        metadata:
          name: ${schema.spec.name}-worker
          namespace: ${schema.metadata.namespace}
        spec:
          template:
            spec:
              bootOptions:
                bootMode: customboot
                custombootConfig:
                  preparingActions:
                    - powerAction: "off"
                    - bootDevice:
                        device: "pxe"
                        efiBoot: true
                    - powerAction: "on"
                  postActions:
                    - powerAction: "off"
                    - bootDevice:
                        device: "disk"
                        persistent: true
                        efiBoot: true
                    - powerAction: "on"
              hardwareAffinity:
                required:
                  - labelSelector:
                      matchLabels:
                        tinkerbell.org/role: worker
              templateOverride: |-
                version: "0.1"
                name: worker
                global_timeout: 9000
                tasks:
                  - name: "worker"
                    worker: "{{.device_1}}"
                    volumes:
                      - /dev:/dev
                      - /dev/console:/dev/console
                      - /lib/firmware:/lib/firmware:ro
                    actions:
                      - name: "Stream Ubuntu Image"
                        image: quay.io/tinkerbell/actions/oci2disk:latest
                        timeout: 3000
                        environment:
                          DEST_DISK: {{ index .Hardware.Disks 0 }}
                          IMG_URL: ${schema.spec.image.registry}/${schema.spec.image.osDistro}-${schema.spec.image.osVersion}:${schema.spec.kubernetesVersion}.gz
                          COMPRESSED: true
                      - name: "Grow Partition"
                        image: quay.io/tinkerbell/actions/cexec:latest
                        timeout: 90
                        environment:
                          BLOCK_DEVICE: {{ index .Hardware.Disks 0 }}3
                          FS_TYPE: ext4
                          CHROOT: y
                          DEFAULT_INTERPRETER: "/bin/sh -c"
                          CMD_LINE: "growpart {{ index .Hardware.Disks 0 }} 3 && resize2fs {{ index .Hardware.Disks 0 }}3"
                      - name: "Add Tink Cloud-Init Config"
                        image: quay.io/tinkerbell/actions/writefile:latest
                        timeout: 90
                        environment:
                          DEST_DISK: {{ formatPartition ( index .Hardware.Disks 0 ) 3 }}
                          FS_TYPE: ext4
                          DEST_PATH: /etc/cloud/cloud.cfg.d/10_tinkerbell.cfg
                          UID: 0
                          GID: 0
                          MODE: 0600
                          DIRMODE: 0700
                          CONTENTS: |
                            datasource:
                              Ec2:
                                metadata_urls: ["http://{{ (index .Hardware.Interfaces 0).DHCP.IP.Gateway }}:7172"]
                                strict_id: false
                            manage_etc_hosts: localhost
                            warnings:
                              dsid_missing_source: off
                      - name: "Add Tink Cloud-Init DS-Identity"
                        image: quay.io/tinkerbell/actions/writefile:latest
                        timeout: 90
                        environment:
                          DEST_DISK: {{ formatPartition ( index .Hardware.Disks 0 ) 3 }}
                          FS_TYPE: ext4
                          DEST_PATH: /etc/cloud/ds-identify.cfg
                          UID: 0
                          GID: 0
                          MODE: 0600
                          DIRMODE: 0700
                          CONTENTS: |
                            datasource: Ec2
                      - name: "Shutdown"
                        image: ghcr.io/jacobweinstock/waitdaemon:latest
                        timeout: 90
                        pid: host
                        command: ["shutdown"]
                        environment:
                          IMAGE: alpine
                          WAIT_SECONDS: 10
                        volumes:
                          - /var/run/docker.sock:/var/run/docker.sock

    # Worker KubeadmConfigTemplate
    - id: workerConfigTemplate
      template:
        apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
        kind: KubeadmConfigTemplate
        metadata:
          name: ${schema.spec.name}-worker
          namespace: ${schema.metadata.namespace}
        spec:
          template:
            spec:
              joinConfiguration:
                nodeRegistration:
                  kubeletExtraArgs:
                    - name: provider-id
                      value: PROVIDER_ID
              users:
                - name: ${schema.spec.user.name}
                  passwd: ${schema.spec.user.passwordHash}
                  lockPassword: ${schema.spec.user.lockPassword}
                  sshAuthorizedKeys: ${schema.spec.user.sshAuthorizedKeys}
                  sudo: ALL=(ALL) NOPASSWD:ALL

    # Worker MachineDeployment
    - id: workerDeployment
      template:
        apiVersion: cluster.x-k8s.io/v1beta2
        kind: MachineDeployment
        metadata:
          name: ${schema.spec.name}-worker
          namespace: ${schema.metadata.namespace}
          labels:
            cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
            nodepool: worker-pool
        spec:
          clusterName: ${cluster.metadata.name}
          replicas: ${schema.spec.workers.replicas}
          selector:
            matchLabels:
              cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
              nodepool: worker-pool
          template:
            metadata:
              labels:
                cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
                nodepool: worker-pool
            spec:
              bootstrap:
                configRef:
                  apiGroup: bootstrap.cluster.x-k8s.io
                  kind: KubeadmConfigTemplate
                  name: ${workerConfigTemplate.metadata.name}
              clusterName: ${cluster.metadata.name}
              infrastructureRef:
                apiGroup: infrastructure.cluster.x-k8s.io
                kind: TinkerbellMachineTemplate
                name: ${workerMachineTemplate.metadata.name}
              version: ${schema.spec.kubernetesVersion}

    # TinkerbellCluster Infrastructure
    - id: tinkerbellCluster
      template:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: TinkerbellCluster
        metadata:
          name: ${schema.spec.name}
          namespace: ${schema.metadata.namespace}
        spec:
          controlPlaneEndpoint:
            host: ${schema.spec.controlPlaneEndpoint.host}.nip.io
            port: ${schema.spec.controlPlaneEndpoint.port}
          imageLookupBaseRegistry: ${schema.spec.image.registry}
          imageLookupFormat: "{{.BaseRegistry}}/{{.OSDistro}}-{{.OSVersion}}:{{.KubernetesVersion}}.gz"
          imageLookupOSDistro: ${schema.spec.image.osDistro}
          imageLookupOSVersion: ${schema.spec.image.osVersion}

    # Cluster
    - id: cluster
      template:
        apiVersion: cluster.x-k8s.io/v1beta2
        kind: Cluster
        metadata:
          name: ${schema.spec.name}
          namespace: ${schema.metadata.namespace}
          labels:
            cluster.x-k8s.io/cluster-name: ${schema.spec.name}
            cni: cilium
        spec:
          clusterNetwork:
            pods:
              cidrBlocks:
                - ${schema.spec.network.podCIDR}
            services:
              cidrBlocks:
                - ${schema.spec.network.serviceCIDR}
          controlPlaneEndpoint:
            host: ${schema.spec.controlPlaneEndpoint.host}.nip.io
            port: ${schema.spec.controlPlaneEndpoint.port}
          controlPlaneRef:
            apiGroup: controlplane.cluster.x-k8s.io
            kind: KubeadmControlPlane
            name: ${schema.spec.name}-control-plane
          infrastructureRef:
            apiGroup: infrastructure.cluster.x-k8s.io
            kind: TinkerbellCluster
            name: ${schema.spec.name}
