# WORKAROUND: HCP bug where konnectivity-agent connects to port 443 instead of 8132.
# See: https://github.com/teutonet/cluster-api-provider-hosted-control-plane/pull/73
#
# Until PR #73 is merged, this RGD implements a routing workaround:
# 1. hcpGatewayService: LoadBalancer that claims gateway.ip and routes to Traefik
#    - Prevents HCP's s-<cluster> service from getting gateway.ip
#    - All external traffic goes through Traefik instead of directly to HCP service
# 2. Gateway listeners on ports 443, 6443, 8132 with TLS passthrough
#    - Traffic on any port is routed by hostname via TLSRoutes
# 3. TLSRoutes (created by HCP controller) route by hostname:
#    - konnectivity.* on port 443 → s-<cluster>:8132 (fixes the bug)
#    - <cluster>.* on port 443/6443 → s-<cluster>:443 (API server)
#
# Requires Traefik entrypoints: hcp (8132), hcpgw (8443→443), hcplegacy (9443→6443)
apiVersion: kro.run/v1alpha1
kind: ResourceGraphDefinition
metadata:
  name: tinkerbellkubeadmhostedcluster.v1alpha1.kro.run
spec:
  schema:
    apiVersion: v1alpha1
    kind: TinkerbellKubeadmHostedCluster
    spec:
      name: string
      namespace: string | default=capi-system
      kubernetesVersion: string | default=v1.34.3
      gateway:
        name: string
        namespace: string | default=capi-system
        ip: string
      traefik:
        namespace: string | default=traefik-system
      kubevip:
        ipRange: string | default=""
      ciliumLB:
        ipRangeStart: string | default=""
        ipRangeStop: string | default=""
      network:
        podCIDR: string | default=192.168.0.0/16
        serviceCIDR: string | default=172.26.0.0/16
      controlPlane:
        replicas: integer | default=1
      workers:
        replicas: integer | default=1
      konnectivityClient:
        replicas: integer | default=1
      image:
        registry: string | default=ghcr.io/s3rj1k/playground
        osDistro: string | default=ubuntu
        osVersion: string | default="2404"
      user:
        name: string | default=tink
        passwordHash: string | default="!"
        lockPassword: boolean | default=false
        sshAuthorizedKeys: "[]string | default=[]"
    status:
      clusterReady: ${cluster.status.conditions.exists(c, c.type == "Ready" && c.status == "True")}
      controlPlaneReady: ${hostedControlPlane.status.ready}
  resources:
    # Gateway for HCP traffic
    - id: gateway
      template:
        apiVersion: gateway.networking.k8s.io/v1
        kind: Gateway
        metadata:
          name: ${schema.spec.gateway.name}
          namespace: ${schema.spec.gateway.namespace}
        spec:
          gatewayClassName: traefik
          listeners:
            - name: tls-passthrough
              protocol: TLS
              port: 8132
              hostname: "*.${schema.spec.gateway.ip}.nip.io"
              tls:
                mode: Passthrough
              allowedRoutes:
                namespaces:
                  from: Same
                kinds:
                  - kind: TLSRoute
            # Workaround for HCP bug: konnectivity-agent uses apiServerServicePort (443)
            # instead of konnectivityServicePort (8132). See PR:
            # https://github.com/teutonet/cluster-api-provider-hosted-control-plane/pull/73
            # Note: ports must match Traefik entrypoints (8443, 9443), not external ports (443, 6443)
            - name: tls-passthrough-8443
              protocol: TLS
              port: 8443
              hostname: "*.${schema.spec.gateway.ip}.nip.io"
              tls:
                mode: Passthrough
              allowedRoutes:
                namespaces:
                  from: Same
                kinds:
                  - kind: TLSRoute
            - name: tls-passthrough-9443
              protocol: TLS
              port: 9443
              hostname: "*.${schema.spec.gateway.ip}.nip.io"
              tls:
                mode: Passthrough
              allowedRoutes:
                namespaces:
                  from: Same
                kinds:
                  - kind: TLSRoute

    # LoadBalancer service to claim gateway IP and route to Traefik.
    # This ensures traffic to gateway.ip goes through Traefik (for TLSRoute routing)
    # instead of directly to the HCP service (s-<cluster-name>) which would bypass Gateway.
    - id: hcpGatewayService
      template:
        apiVersion: v1
        kind: Service
        metadata:
          name: hcp-gateway-${schema.spec.name}
          namespace: ${schema.spec.traefik.namespace}
          annotations:
            kube-vip.io/loadbalancerIPs: ${schema.spec.gateway.ip}
            io.cilium/lb-ipam-ips: ${schema.spec.gateway.ip}
        spec:
          type: LoadBalancer
          selector:
            app.kubernetes.io/name: traefik
          ports:
            - name: hcp
              port: 8132
              targetPort: hcp
            - name: hcpgw
              port: 443
              targetPort: hcpgw
            - name: hcplegacy
              port: 6443
              targetPort: hcplegacy

    # TLSRoute for legacy API server port (6443)
    # HCP controller only creates TLSRoutes for ports 443 and 8132, but cilium uses 6443
    - id: apiServerLegacyTLSRoute
      template:
        apiVersion: gateway.networking.k8s.io/v1alpha2
        kind: TLSRoute
        metadata:
          name: ${schema.spec.name}-api-legacy
          namespace: ${schema.spec.namespace}
          labels:
            cluster.x-k8s.io/cluster-name: ${schema.spec.name}
        spec:
          parentRefs:
            - group: gateway.networking.k8s.io
              kind: Gateway
              name: ${schema.spec.gateway.name}
              namespace: ${schema.spec.gateway.namespace}
          hostnames:
            - "${schema.spec.name}.${schema.spec.namespace}.${schema.spec.gateway.ip}.nip.io"
          rules:
            - backendRefs:
                - name: s-${schema.spec.name}
                  port: 443

    # Kube-vip IP pool ConfigMap
    - id: kubevipConfigMap
      includeWhen:
        - ${schema.spec.kubevip.ipRange != ""}
      template:
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: kube-vip-cloud-provider
          namespace: ${schema.spec.namespace}
        data:
          range-global: ${schema.spec.kubevip.ipRange}

    # CiliumLoadBalancerIPPool for HCP when kube-vip is disabled
    - id: ciliumLBIPPool
      includeWhen:
        - ${schema.spec.kubevip.ipRange == "" && schema.spec.ciliumLB.ipRangeStart != ""}
      template:
        apiVersion: cilium.io/v2
        kind: CiliumLoadBalancerIPPool
        metadata:
          name: ${schema.spec.name}-hcp-pool
        spec:
          blocks:
            - start: ${schema.spec.ciliumLB.ipRangeStart}
              stop: ${schema.spec.ciliumLB.ipRangeStop}

    # Cluster
    - id: cluster
      template:
        apiVersion: cluster.x-k8s.io/v1beta2
        kind: Cluster
        metadata:
          name: ${schema.spec.name}
          namespace: ${schema.spec.namespace}
          labels:
            cluster.x-k8s.io/cluster-name: ${schema.spec.name}
            cni: cilium
        spec:
          clusterNetwork:
            pods:
              cidrBlocks:
                - ${schema.spec.network.podCIDR}
            services:
              cidrBlocks:
                - ${schema.spec.network.serviceCIDR}
          controlPlaneRef:
            apiGroup: controlplane.cluster.x-k8s.io
            kind: HostedControlPlane
            name: ${schema.spec.name}-control-plane
          infrastructureRef:
            apiGroup: infrastructure.cluster.x-k8s.io
            kind: TinkerbellCluster
            name: ${schema.spec.name}

    # TinkerbellCluster Infrastructure
    - id: tinkerbellCluster
      template:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: TinkerbellCluster
        metadata:
          name: ${schema.spec.name}
          namespace: ${schema.spec.namespace}
        spec:
          imageLookupBaseRegistry: ${schema.spec.image.registry}
          imageLookupFormat: "{{.BaseRegistry}}/{{.OSDistro}}-{{.OSVersion}}:{{.KubernetesVersion}}.gz"
          imageLookupOSDistro: ${schema.spec.image.osDistro}
          imageLookupOSVersion: ${schema.spec.image.osVersion}

    # HostedControlPlane
    - id: hostedControlPlane
      template:
        apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
        kind: HostedControlPlane
        metadata:
          name: ${schema.spec.name}-control-plane
          namespace: ${schema.spec.namespace}
        spec:
          version: ${schema.spec.kubernetesVersion}
          replicas: ${schema.spec.controlPlane.replicas}
          gateway:
            namespace: ${schema.spec.gateway.namespace}
            name: ${schema.spec.gateway.name}
          deployment:
            controllerManager:
              args:
                allocate-node-cidrs: "true"
          konnectivityClient:
            replicas: ${schema.spec.konnectivityClient.replicas}
          kubeProxy: {}
          coredns: {}

    # Cilium HelmChartProxy
    - id: ciliumAddon
      template:
        apiVersion: addons.cluster.x-k8s.io/v1alpha1
        kind: HelmChartProxy
        metadata:
          name: cilium-${schema.spec.name}
          namespace: ${schema.spec.namespace}
        spec:
          clusterSelector:
            matchLabels:
              cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
          repoURL: https://helm.cilium.io
          chartName: cilium
          namespace: kube-system
          releaseName: cilium
          valuesTemplate: |
            ipam:
              mode: kubernetes
            k8sServiceHost: ${schema.spec.name}.${schema.spec.namespace}.${schema.spec.gateway.ip}.nip.io
            k8sServicePort: 6443
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
              - key: node.cluster.x-k8s.io/uninitialized
                operator: Exists
                effect: NoSchedule
              - key: node.kubernetes.io/not-ready
                operator: Exists
                effect: NoSchedule
            operator:
              replicas: ${string(schema.spec.workers.replicas)}
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                - key: node.cluster.x-k8s.io/uninitialized
                  operator: Exists
                  effect: NoSchedule
                - key: node.kubernetes.io/not-ready
                  operator: Exists
                  effect: NoSchedule
            envoy:
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                - key: node.cluster.x-k8s.io/uninitialized
                  operator: Exists
                  effect: NoSchedule
                - key: node.kubernetes.io/not-ready
                  operator: Exists
                  effect: NoSchedule

    # Worker TinkerbellMachineTemplate
    - id: workerMachineTemplate
      template:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: TinkerbellMachineTemplate
        metadata:
          name: ${schema.spec.name}-worker
          namespace: ${schema.spec.namespace}
        spec:
          template:
            spec:
              bootOptions:
                bootMode: customboot
                custombootConfig:
                  preparingActions:
                    - powerAction: "off"
                    - bootDevice:
                        device: "pxe"
                        efiBoot: true
                    - powerAction: "on"
                  postActions:
                    - powerAction: "off"
                    - bootDevice:
                        device: "disk"
                        persistent: true
                        efiBoot: true
                    - powerAction: "on"
              hardwareAffinity:
                required:
                  - labelSelector:
                      matchLabels:
                        tinkerbell.org/role: worker
              templateOverride: |-
                version: "0.1"
                name: worker
                global_timeout: 9000
                tasks:
                  - name: "worker"
                    worker: "{{.device_1}}"
                    volumes:
                      - /dev:/dev
                      - /dev/console:/dev/console
                      - /lib/firmware:/lib/firmware:ro
                    actions:
                      - name: "Stream Ubuntu Image"
                        image: quay.io/tinkerbell/actions/oci2disk:latest
                        timeout: 3000
                        environment:
                          DEST_DISK: {{ index .Hardware.Disks 0 }}
                          IMG_URL: ${schema.spec.image.registry}/${schema.spec.image.osDistro}-${schema.spec.image.osVersion}:${schema.spec.kubernetesVersion}.gz
                          COMPRESSED: true
                      - name: "Grow Partition"
                        image: quay.io/tinkerbell/actions/cexec:latest
                        timeout: 90
                        environment:
                          BLOCK_DEVICE: {{ index .Hardware.Disks 0 }}3
                          FS_TYPE: ext4
                          CHROOT: y
                          DEFAULT_INTERPRETER: "/bin/sh -c"
                          CMD_LINE: "growpart {{ index .Hardware.Disks 0 }} 3 && resize2fs {{ index .Hardware.Disks 0 }}3"
                      - name: "Add Tink Cloud-Init Config"
                        image: quay.io/tinkerbell/actions/writefile:latest
                        timeout: 90
                        environment:
                          DEST_DISK: {{ formatPartition ( index .Hardware.Disks 0 ) 3 }}
                          FS_TYPE: ext4
                          DEST_PATH: /etc/cloud/cloud.cfg.d/10_tinkerbell.cfg
                          UID: 0
                          GID: 0
                          MODE: 0600
                          DIRMODE: 0700
                          CONTENTS: |
                            datasource:
                              Ec2:
                                metadata_urls: ["http://{{ (index .Hardware.Interfaces 0).DHCP.IP.Gateway }}:7172"]
                                strict_id: false
                            manage_etc_hosts: localhost
                            warnings:
                              dsid_missing_source: off
                      - name: "Add Tink Cloud-Init DS-Identity"
                        image: quay.io/tinkerbell/actions/writefile:latest
                        timeout: 90
                        environment:
                          DEST_DISK: {{ formatPartition ( index .Hardware.Disks 0 ) 3 }}
                          FS_TYPE: ext4
                          DEST_PATH: /etc/cloud/ds-identify.cfg
                          UID: 0
                          GID: 0
                          MODE: 0600
                          DIRMODE: 0700
                          CONTENTS: |
                            datasource: Ec2
                      - name: "Shutdown"
                        image: ghcr.io/jacobweinstock/waitdaemon:latest
                        timeout: 90
                        pid: host
                        command: ["shutdown"]
                        environment:
                          IMAGE: alpine
                          WAIT_SECONDS: 10
                        volumes:
                          - /var/run/docker.sock:/var/run/docker.sock

    # Worker KubeadmConfigTemplate
    - id: workerConfigTemplate
      template:
        apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
        kind: KubeadmConfigTemplate
        metadata:
          name: ${schema.spec.name}-worker
          namespace: ${schema.spec.namespace}
        spec:
          template:
            spec:
              joinConfiguration:
                nodeRegistration:
                  kubeletExtraArgs:
                    - name: provider-id
                      value: PROVIDER_ID
              users:
                - name: ${schema.spec.user.name}
                  passwd: ${schema.spec.user.passwordHash}
                  lockPassword: ${schema.spec.user.lockPassword}
                  sshAuthorizedKeys: ${schema.spec.user.sshAuthorizedKeys}
                  sudo: ALL=(ALL) NOPASSWD:ALL

    # Worker MachineDeployment
    - id: workerDeployment
      template:
        apiVersion: cluster.x-k8s.io/v1beta2
        kind: MachineDeployment
        metadata:
          name: ${schema.spec.name}-worker
          namespace: ${schema.spec.namespace}
          labels:
            cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
            nodepool: worker-pool
        spec:
          clusterName: ${cluster.metadata.name}
          replicas: ${schema.spec.workers.replicas}
          selector:
            matchLabels:
              cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
              nodepool: worker-pool
          template:
            metadata:
              labels:
                cluster.x-k8s.io/cluster-name: ${cluster.metadata.name}
                nodepool: worker-pool
            spec:
              bootstrap:
                configRef:
                  apiGroup: bootstrap.cluster.x-k8s.io
                  kind: KubeadmConfigTemplate
                  name: ${workerConfigTemplate.metadata.name}
              clusterName: ${cluster.metadata.name}
              infrastructureRef:
                apiGroup: infrastructure.cluster.x-k8s.io
                kind: TinkerbellMachineTemplate
                name: ${workerMachineTemplate.metadata.name}
              version: ${schema.spec.kubernetesVersion}
